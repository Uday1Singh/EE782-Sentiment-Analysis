{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqwBkQRNbjGS"
   },
   "source": [
    "Uday Singh (22B1262) \\\n",
    "Ankit Maurya (22B1266) \\\n",
    "Aditya Bhadoria (22B1247)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiwAiYDmJAmv"
   },
   "source": [
    "Github - https://github.com/Uday1Singh/EE782-Sentiment-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBwYZR-cJAvn",
    "outputId": "b7817859-e33b-485e-e7d1-38be14df74c0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WixGh1gt3TN5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jG74zSAqfqoS"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Paths - UPDATE THESE TO YOUR PATHS\n",
    "    SPEECH_DIR = r'/content/drive/MyDrive/audio_speech_actors_01-24'\n",
    "    # SONG_DIR = r'C:\\Users\\pashu\\OneDrive\\Desktop\\non-academics\\soc\\speech_recong_project\\Audio_Song_Actors_01-24'\n",
    "\n",
    "    # Model\n",
    "    PRETRAINED_MODEL = \"facebook/wav2vec2-base\"\n",
    "\n",
    "    # RAVDESS Emotion Mapping (based on filename position 7)\n",
    "    # Position 7 in RAVDESS filename indicates emotion\n",
    "    EMOTION_MAP = {\n",
    "        '1': 'neutral',\n",
    "        '2': 'calm',\n",
    "        '3': 'happy',\n",
    "        '4': 'sad',\n",
    "        '5': 'angry',\n",
    "        '6': 'fearful',\n",
    "        '7': 'disgusted',\n",
    "        '8': 'surprised'\n",
    "    }\n",
    "\n",
    "    # Training\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 3e-5\n",
    "    NUM_EPOCHS = 15\n",
    "    SAMPLE_RATE = 16000\n",
    "    MAX_AUDIO_LENGTH = 5  # seconds\n",
    "    TEST_SIZE = 0.2\n",
    "\n",
    "    # Save paths\n",
    "    MODEL_SAVE_PATH = 'emotion_transformer_model.pth'\n",
    "    LABEL_ENCODER_PATH = 'label_encoder_transformer.pkl'\n",
    "    PROCESSOR_PATH = 'wav2vec2_processor'\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKgXgTo0ILRX"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET CLASS FOR RAVDESS (ROBUST FIX)\n",
    "# ============================================================================\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class RAVDESSDataset(Dataset):\n",
    "    \"\"\"Dataset for RAVDESS audio files with Transformer preprocessing\"\"\"\n",
    "\n",
    "    def __init__(self, file_paths, labels, processor, max_length_seconds=5):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "        self.max_samples = int(max_length_seconds * config.SAMPLE_RATE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            # 1. LOAD AUDIO WITH LIBROSA\n",
    "            #    More robust than torchaudio for Colab/Drive\n",
    "            waveform, sr = librosa.load(file_path, sr=config.SAMPLE_RATE, mono=True)\n",
    "\n",
    "            # 2. PAD OR TRUNCATE\n",
    "            if len(waveform) < self.max_samples:\n",
    "                padding = self.max_samples - len(waveform)\n",
    "                waveform = np.pad(waveform, (0, padding), 'constant')\n",
    "            else:\n",
    "                waveform = waveform[:self.max_samples]\n",
    "\n",
    "            # 3. PROCESS WITH WAV2VEC2\n",
    "            inputs = self.processor(\n",
    "                waveform,\n",
    "                sampling_rate=config.SAMPLE_RATE,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True # Ensure mask is created\n",
    "            )\n",
    "\n",
    "            # 4. ROBUST MASK RETRIEVAL\n",
    "            input_values = inputs['input_values']\n",
    "            if 'attention_mask' in inputs:\n",
    "                attention_mask = inputs['attention_mask']\n",
    "            else:\n",
    "                # Fallback if processor doesn't return mask\n",
    "                attention_mask = torch.ones_like(input_values)\n",
    "\n",
    "            return {\n",
    "                'input_values': input_values.squeeze(),\n",
    "                'attention_mask': attention_mask.squeeze(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            # PRINT ERROR BUT DO NOT CRASH\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "            # Fallback: return dummy zeros\n",
    "            dummy_audio = np.zeros(self.max_samples)\n",
    "            inputs = self.processor(\n",
    "                dummy_audio,\n",
    "                sampling_rate=config.SAMPLE_RATE,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "\n",
    "            # Safely get/create mask for dummy data\n",
    "            input_values = inputs['input_values']\n",
    "            if 'attention_mask' in inputs:\n",
    "                attention_mask = inputs['attention_mask']\n",
    "            else:\n",
    "                attention_mask = torch.ones_like(input_values)\n",
    "\n",
    "            return {\n",
    "                'input_values': input_values.squeeze(),\n",
    "                'attention_mask': attention_mask.squeeze(),\n",
    "                'labels': torch.tensor(0, dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwVifmiCfqqs"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# LOAD RAVDESS DATA (YOUR FORMAT)\n",
    "# ============================================================================\n",
    "\n",
    "def load_ravdess_data():\n",
    "    \"\"\"Load data from your RAVDESS folders using os.walk for robust traversal\"\"\"\n",
    "\n",
    "    file_paths = []\n",
    "    emotions = []\n",
    "\n",
    "    # Load from both Speech and Song directories\n",
    "    for directory in [config.SPEECH_DIR]:\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"Warning: Directory not found: {directory}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Loading from: {directory}\")\n",
    "\n",
    "        # Use os.walk to recursively search for files in all subdirectories\n",
    "        for root, _, filenames in os.walk(directory):\n",
    "            for filename in filenames:\n",
    "                # Check for .wav files (case-insensitive for robustness)\n",
    "                if filename.lower().endswith('.wav'):\n",
    "                    file_path = os.path.join(root, filename)\n",
    "\n",
    "                    # Extract emotion from filename (position 7)\n",
    "                    # RAVDESS format: 03-01-06-01-02-01-12.wav\n",
    "                    try:\n",
    "                        # Position 3 (0-indexed: 2) is the emotion code (e.g., '06')\n",
    "                        emotion_code = filename.split('-')[2]\n",
    "\n",
    "                        # Convert '01'/'08' to '1'/'8' for correct dictionary lookup\n",
    "                        emotion_code_key = str(int(emotion_code))\n",
    "\n",
    "                        emotion = config.EMOTION_MAP.get(emotion_code_key, 'unknown')\n",
    "\n",
    "                        if emotion != 'unknown':\n",
    "                            file_paths.append(file_path)\n",
    "                            emotions.append(emotion)\n",
    "                    except:\n",
    "                        print(f\"Could not parse filename: {filename}\")\n",
    "\n",
    "    print(f\"\\nTotal files loaded: {len(file_paths)}\")\n",
    "    print(f\"Emotion distribution:\")\n",
    "    for emotion in set(emotions):\n",
    "        count = emotions.count(emotion)\n",
    "        print(f\"  {emotion}: {count}\")\n",
    "\n",
    "    return file_paths, emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-zrgskzfqtG"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRANSFORMER MODEL (UNCHANGED)\n",
    "# ============================================================================\n",
    "\n",
    "class EmotionTransformer(nn.Module):\n",
    "    \"\"\"Transformer model for emotion recognition\"\"\"\n",
    "\n",
    "    def __init__(self, num_emotions=8):\n",
    "        super(EmotionTransformer, self).__init__()\n",
    "\n",
    "        # Load Wav2Vec2\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(config.PRETRAINED_MODEL)\n",
    "\n",
    "        # ========================================================================\n",
    "        # THE UPDATE: UNFREEZE THE TRANSFORMER LAYERS\n",
    "        # ========================================================================\n",
    "\n",
    "        # Instead of freezing everything, we ONLY freeze the feature extractor.\n",
    "        # The feature extractor converts raw audio to latent features. It is fragile\n",
    "        # and usually kept frozen during fine-tuning.\n",
    "        self.wav2vec2.feature_extractor.requires_grad_(False)\n",
    "\n",
    "        # The rest of the model (the Transformer encoder) remains UNFROZEN (requires_grad=True).\n",
    "        # This allows it to learn \"How\" you are speaking (emotions), not just \"What\" (words).\n",
    "\n",
    "        # (Deleted the loop that set everything to False)\n",
    "        # ========================================================================\n",
    "\n",
    "        hidden_size = self.wav2vec2.config.hidden_size\n",
    "\n",
    "        # Attention pooling (Unchanged)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Classification head (Unchanged)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_emotions)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None):\n",
    "        # Extract features\n",
    "        outputs = self.wav2vec2(input_values, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Attention pooling\n",
    "        attention_weights = self.attention(hidden_states)\n",
    "        pooled = torch.sum(hidden_states * attention_weights, dim=1)\n",
    "\n",
    "        # Classify\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTION (FIXED: num_workers=0)\n",
    "# ============================================================================\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "    # Load processor\n",
    "    print(\"\\nLoading Wav2Vec2 processor...\")\n",
    "    processor = Wav2Vec2Processor.from_pretrained(config.PRETRAINED_MODEL)\n",
    "\n",
    "    # Load data\n",
    "    print(\"\\nLoading RAVDESS data...\")\n",
    "    file_paths, emotions = load_ravdess_data()\n",
    "\n",
    "    if len(file_paths) == 0:\n",
    "        print(\"ERROR: No data loaded! Check your directory paths.\")\n",
    "        return\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_encoded = label_encoder.fit_transform(emotions)\n",
    "\n",
    "    # Save label encoder\n",
    "    with open(config.LABEL_ENCODER_PATH, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "\n",
    "    print(f\"\\nEmotions: {label_encoder.classes_}\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        file_paths, labels_encoded,\n",
    "        test_size=config.TEST_SIZE,\n",
    "        random_state=42,\n",
    "        stratify=labels_encoded\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTrain samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = RAVDESSDataset(X_train, y_train, processor)\n",
    "    test_dataset = RAVDESSDataset(X_test, y_test, processor)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0 # SET TO 0 FOR STABLE GOOGLE DRIVE I/O\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0 # SET TO 0 FOR STABLE GOOGLE DRIVE I/O\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"\\nInitializing Transformer model...\")\n",
    "    num_emotions = len(label_encoder.classes_)\n",
    "    model = EmotionTransformer(num_emotions=num_emotions).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "    # Training loop\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, config.NUM_EPOCHS + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{config.NUM_EPOCHS} [Train]')\n",
    "        for batch in pbar:\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(input_values, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100 * train_correct / train_total:.2f}%'\n",
    "            })\n",
    "\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "\n",
    "        # Test\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_values = batch['input_values'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                logits = model(input_values, attention_mask)\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * test_correct / test_total\n",
    "\n",
    "        print(f'\\nEpoch {epoch}:')\n",
    "        print(f'  Train Accuracy: {train_acc:.2f}%')\n",
    "        print(f'  Test Accuracy: {test_acc:.2f}%')\n",
    "\n",
    "        # Save best model\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'test_acc': test_acc,\n",
    "                'label_encoder': label_encoder\n",
    "            }, config.MODEL_SAVE_PATH)\n",
    "            print(f'  âœ“ Best model saved! (Accuracy: {test_acc:.2f}%)')\n",
    "\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING COMPLETE!\")\n",
    "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
    "    print(f\"Your old model: 74.75%\")\n",
    "    print(f\"Improvement: +{best_acc - 74.75:.2f}%\")\n",
    "    print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaVjX-Prfqvu"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INFERENCE (FOR YOUR GUI)\n",
    "# ============================================================================\n",
    "\n",
    "def load_trained_model():\n",
    "    \"\"\"Load trained model for inference\"\"\"\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(config.MODEL_SAVE_PATH, map_location=device)\n",
    "    label_encoder = checkpoint['label_encoder']\n",
    "\n",
    "    # Initialize model\n",
    "    model = EmotionTransformer(num_emotions=len(label_encoder.classes_))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load processor\n",
    "    processor = Wav2Vec2Processor.from_pretrained(config.PRETRAINED_MODEL)\n",
    "\n",
    "    return model, processor, label_encoder, device\n",
    "\n",
    "def predict_emotion_transformer(audio, model, processor, label_encoder, device):\n",
    "    \"\"\"\n",
    "    Predict emotion from audio array\n",
    "    Compatible with your existing GUI code\n",
    "\n",
    "    Args:\n",
    "        audio: numpy array of audio samples\n",
    "        model: trained model\n",
    "        processor: Wav2Vec2 processor\n",
    "        label_encoder: fitted label encoder\n",
    "        device: torch device\n",
    "\n",
    "    Returns:\n",
    "        emotion: predicted emotion string\n",
    "        confidence: confidence score\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to mono if stereo\n",
    "    if audio.ndim > 1:\n",
    "        audio = audio.mean(axis=1)\n",
    "\n",
    "    # Process audio\n",
    "    inputs = processor(\n",
    "        audio,\n",
    "        sampling_rate=config.SAMPLE_RATE,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    input_values = inputs.input_values.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values, attention_mask)\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        predicted_idx = torch.argmax(probabilities, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_idx].item()\n",
    "\n",
    "    emotion = label_encoder.inverse_transform([predicted_idx])[0]\n",
    "\n",
    "    return emotion, confidence\n",
    "\n",
    "# ============================================================================\n",
    "# GUI INTEGRATION CODE\n",
    "# ============================================================================\n",
    "\n",
    "def create_gui_with_transformer():\n",
    "    \"\"\"\n",
    "    Updated GUI code that uses Transformer model\n",
    "    Drop-in replacement for your existing GUI\n",
    "    \"\"\"\n",
    "\n",
    "    import tkinter as tk\n",
    "    from tkinter import messagebox\n",
    "    import sounddevice as sd\n",
    "    import librosa\n",
    "\n",
    "    # Load model once\n",
    "    print(\"Loading Transformer model...\")\n",
    "    model, processor, label_encoder, device = load_trained_model()\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "    def record_audio(duration=5, fs=16000):  # Changed to 16kHz for Wav2Vec2\n",
    "        print(\"Recording...\")\n",
    "        audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='float32')\n",
    "        sd.wait()\n",
    "        print(\"Recording complete.\")\n",
    "        return audio.flatten()\n",
    "\n",
    "    def predict_emotion():\n",
    "        try:\n",
    "            # Record audio\n",
    "            audio = record_audio(duration=5)\n",
    "\n",
    "            # Resample if needed (ensure 16kHz)\n",
    "            if len(audio) != 5 * 16000:\n",
    "                audio = librosa.resample(audio, orig_sr=44100, target_sr=16000)\n",
    "\n",
    "            # Predict\n",
    "            emotion, confidence = predict_emotion_transformer(\n",
    "                audio, model, processor, label_encoder, device\n",
    "            )\n",
    "\n",
    "            messagebox.showinfo(\n",
    "                \"Predicted Emotion\",\n",
    "                f'Emotion: {emotion.capitalize()}\\n'\n",
    "                f'Confidence: {confidence*100:.1f}%'\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Prediction failed: {str(e)}\")\n",
    "\n",
    "    # Create GUI\n",
    "    app = tk.Tk()\n",
    "    app.title(\"Transformer Emotion Detection\")\n",
    "    app.geometry(\"350x200\")\n",
    "\n",
    "    title_label = tk.Label(\n",
    "        app,\n",
    "        text=\"Speech Emotion Recognition\\n(Transformer Model)\",\n",
    "        font=(\"Arial\", 12, \"bold\")\n",
    "    )\n",
    "    title_label.pack(pady=20)\n",
    "\n",
    "    record_button = tk.Button(\n",
    "        app,\n",
    "        text=\"ðŸŽ¤ Record and Predict Emotion\",\n",
    "        command=predict_emotion,\n",
    "        height=2,\n",
    "        width=30,\n",
    "        bg=\"#4CAF50\",\n",
    "        fg=\"white\",\n",
    "        font=(\"Arial\", 10, \"bold\")\n",
    "    )\n",
    "    record_button.pack(pady=20)\n",
    "\n",
    "    info_label = tk.Label(\n",
    "        app,\n",
    "        text=\"Speak for 5 seconds when recording starts\",\n",
    "        font=(\"Arial\", 9),\n",
    "        fg=\"gray\"\n",
    "    )\n",
    "    info_label.pack()\n",
    "\n",
    "    app.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4704cd262ee7438bb637a06466575cbf",
      "179b8fb6b05d4e2c8cf98ef15c59974e",
      "f1407ab3aa89478c88bb69af76050331",
      "b45fccb363694cd59d544d67c6650148",
      "cd17771271df4de68257d8d75cce9016",
      "53edb46f8cab4d1c9b845a4a81289e29",
      "a67916fde54f430493853fc3ff69f097",
      "6c6e9677779543e6894f77ece91d6351",
      "d6a0346dc944418bb8a6692943ec8136",
      "efe35a7cbef84fcbb4cca0182715acc6",
      "b1623add32034808861d9af04bd5c48d",
      "27675be894df4728a03889886c3dcdbb",
      "e4414dbcca9743378a388f6280559b9a",
      "a241fccfdd484097af98450b97ac705f",
      "806ada9420d6471caa454dea4ba09db2",
      "d1caa9dface545d0b69c674d657d2535",
      "1cf73fbbf77c41cbb1b36f8037cda3ac",
      "7c9d3f37e3684730899fb3c79e08e155",
      "b9bd84c44a1d45deb837da23dddee369",
      "3550cab2dd7a4fdbb38d8bbfd0277626",
      "8c984e6a321b44bfb42979a24a8d6a7f",
      "3272f6cdb8bc45b1b3729f3e4421d3ad",
      "b2b3717e0084491db2e7bd669e89d1b7",
      "21dcfcf7348b4fa791a2f98ce1e5ed1f",
      "6cf441bbdb09403a86e5865151439654",
      "901b65294d6b4e13b7b8809978f08134",
      "4c7bdae74e0945568c81f0f6377d7dc1",
      "d67f22c9545545f9b17bb6778eccc48d",
      "cdcfbeb4e547406cbbc104fcba22298a",
      "635e15cef97c4a46902dffd8270befd3",
      "6c810f0906ff4e12bcdf9c140927fc1c",
      "f98c7bfe20944941ae7126ef4a000c57",
      "a5916f559b424ba6a284ef9b1b871f28",
      "0f5877affc7849b2bfba3c04444de1b7",
      "94a364f7768d4ead805178142dda4475",
      "16a24eec9743410ebaa1b015e5d15f10",
      "f947a1bba7594264b0b45c89e9b78561",
      "4d267b744cd74c18b2db74d0c7860eb5",
      "f714b7cf5171429f975ca3b9b8d73e94",
      "69ca02a0a3de4a5586a1128074a38cfc",
      "b76ba5b9740940778c5d783764ca1c4c",
      "ef0c067ba86e48bfad99a42d5ce7e567",
      "57209e9fc9724cd89abbccf9cc978fd8",
      "8cce9f936c00430f89a2b44d267bffeb",
      "9b238df2482f46bba2c601eb89a2cbc8",
      "08508e0635c2409197d923f4eab4c0ac",
      "148478dcce8e43c6837a91082bb40086",
      "1f5ebc6ad55a46c3bbdfe787769bb275",
      "f3a8f4b96b1e460280a8f48074b0903b",
      "5397ffae954a43dcb41643c99df1fc00",
      "9698eed2de4d4f1f9f5709b371e0b843",
      "6ac01ff72ac9491880e4b525050a2da1",
      "0fd7df2090f24fabb0d8a1ed6120545b",
      "f08cc90d0aee410799c7595a294e7a48",
      "72c310aba5be4a7fb755ac824dbc2af1",
      "706d554608fc46b18a08adc5bf63a6ec",
      "43863985bc584f0bb1e9754d6ee7fb01",
      "63cec2386d8549f6be3b7d942b7c2f97",
      "c204f0523950475dbd4748dbd49004f7",
      "28cf6acabfc541e4a02f27dd8f085b2e",
      "dd0dd07ece3b400bb82255590286f731",
      "f2c9f3e5c7f944629229664661e33a5b",
      "0829bbf17100435f924709f38b0885cd",
      "6c54efac1d1148b283e8609d4fce64b7",
      "848f04b8596d436090c0049f1f47058a",
      "80af5f7e49e142c18eac25b7a20efc75",
      "d83d535d122d4e43b76ae1f1ad3a527e",
      "f40361817ad14c4c8fee521adfd8e920",
      "c233d900f6d842b191fb2446d4039722",
      "aa44ae1c530c440a901809a77176bfb8",
      "fb61939461cc41c9abd250d064819ba2",
      "33f55513704645438f7d22cda5e7e12b",
      "8d3406a07cd14bcab609f3ce12573d39",
      "10e91e03302648dd93551161c6812cbc",
      "60b8272b49144fc9bd50ee9da8342917",
      "3ceda121660e48698ef88fc649135ca0",
      "c10a30af24204b908b05bd274ed9029a"
     ]
    },
    "id": "Gv-oGI4L3TQX",
    "outputId": "341c1576-8c8f-4acb-d7bf-22959d0894d8"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Transformer Speech Emotion Recognition')\n",
    "    parser.add_argument('--mode', type=str, default='train',\n",
    "                       choices=['train', 'gui'],\n",
    "                       help='Mode: train model or run GUI')\n",
    "\n",
    "    # FIX FOR JUPYTER NOTEBOOK\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    if args.mode == 'train':\n",
    "        print(\"=\"*80)\n",
    "        print(\"UPGRADING YOUR MODEL TO TRANSFORMERS\")\n",
    "        print(\"=\"*80)\n",
    "        train_model()\n",
    "\n",
    "    elif args.mode == 'gui':\n",
    "        create_gui_with_transformer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ix8Ci4FpKLI",
    "outputId": "163e3357-9e16-4074-e01c-23bd5476a47c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import Wav2Vec2Processor\n",
    "from torch import nn\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "# --- Configuration (Necessary Imports for this block) ---\n",
    "# Assuming 'config' is available and EmotionTransformer is defined.\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 1. FIXED MODEL LOADING FUNCTION (Cell 8)\n",
    "# --------------------------------------------------------\n",
    "def load_trained_model():\n",
    "    \"\"\"Load trained model for inference, applying the weights_only=False fix.\"\"\"\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # FIX: Add weights_only=False for safe loading of LabelEncoder\n",
    "    # Note: Assumes config.MODEL_SAVE_PATH exists and has been saved by Cell 7.\n",
    "    checkpoint = torch.load(config.MODEL_SAVE_PATH, map_location=device, weights_only=False)\n",
    "    label_encoder = checkpoint['label_encoder']\n",
    "\n",
    "    # Initialize model (assumes EmotionTransformer is defined in runtime)\n",
    "    num_emotions = len(label_encoder.classes_)\n",
    "    model = EmotionTransformer(num_emotions=num_emotions)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    processor = Wav2Vec2Processor.from_pretrained(config.PRETRAINED_MODEL)\n",
    "\n",
    "    return model, processor, label_encoder, device\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2. PREDICTION FUNCTION (Cell 8) - FINAL FIX: Safe Mask Access\n",
    "# --------------------------------------------------------\n",
    "def predict_emotion_transformer(audio, model, processor, label_encoder, device):\n",
    "    \"\"\"Predict emotion from audio array using the loaded model.\"\"\"\n",
    "\n",
    "    if audio.ndim > 1:\n",
    "        audio = audio.mean(axis=1)\n",
    "\n",
    "    inputs = processor(\n",
    "        audio,\n",
    "        sampling_rate=config.SAMPLE_RATE,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    input_values = inputs['input_values'].to(device)\n",
    "\n",
    "    # --- FINAL FIX: Safely retrieve attention_mask or create a fallback ---\n",
    "    attention_mask = inputs.get('attention_mask')\n",
    "    if attention_mask is None:\n",
    "        # If mask is missing, assume full attention (all ones) for the input length\n",
    "        attention_mask = torch.ones_like(input_values)\n",
    "    else:\n",
    "        attention_mask = attention_mask.to(device)\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values, attention_mask)\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        predicted_idx = torch.argmax(probabilities, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_idx].item()\n",
    "\n",
    "    emotion = label_encoder.inverse_transform([predicted_idx])[0]\n",
    "\n",
    "    return emotion, confidence\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3. EXECUTION BLOCK\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Load the trained model components\n",
    "try:\n",
    "    model, processor, label_encoder, device = load_trained_model()\n",
    "    print(f\"Model successfully loaded and ready on device: {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR: Failed to load trained model. Details: {e}\")\n",
    "    model = None\n",
    "\n",
    "\n",
    "if model is not None:\n",
    "    # NOTE: You must replace this path with the actual location of one of your .wav files.\n",
    "    SAMPLE_AUDIO_PATH = \"/content/drive/MyDrive/audio_speech_actors_01-24/Actor_01/03-01-01-01-01-01-01.wav\"\n",
    "\n",
    "    print(f\"\\nTesting model with sample file: {SAMPLE_AUDIO_PATH}\")\n",
    "\n",
    "    try:\n",
    "        # Load and preprocess the audio file using LIBROSA\n",
    "        audio_data, sr = librosa.load(\n",
    "            SAMPLE_AUDIO_PATH,\n",
    "            sr=config.SAMPLE_RATE,\n",
    "            mono=True\n",
    "        )\n",
    "\n",
    "        # Run prediction\n",
    "        emotion, confidence = predict_emotion_transformer(\n",
    "            audio_data, model, processor, label_encoder, device\n",
    "        )\n",
    "\n",
    "        print(\"-\" * 30)\n",
    "        print(\"PREDICTION RESULT:\")\n",
    "        print(f\"File: {os.path.basename(SAMPLE_AUDIO_PATH)}\")\n",
    "        print(f\"Predicted Emotion: {emotion.upper()}\")\n",
    "        print(f\"Confidence: {confidence * 100:.2f}%\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nERROR: File not found at {SAMPLE_AUDIO_PATH}\")\n",
    "    except Exception as e:\n",
    "        # If this still fails, it's likely a memory or deep PyTorch error\n",
    "        print(f\"\\nFATAL INFERENCE ERROR:\")\n",
    "        print(f\"Exception Type: {type(e).__name__}\")\n",
    "        print(f\"Error Message: {e}\")\n",
    "        print(\"-\" * 30)\n",
    "        print(\"A critical error occurred during the prediction process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sihFuVwaTX6z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def generate_report_graphs(model, test_loader, device, label_encoder):\n",
    "    \"\"\"\n",
    "    Generates Confusion Matrix and Class-wise Performance Report\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"Running inference on test set... (This might take a minute)\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            # Safe mask retrieval\n",
    "            if 'attention_mask' in batch:\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "            else:\n",
    "                attention_mask = torch.ones_like(input_values).to(device)\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_values, attention_mask)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Get Class Names\n",
    "    class_names = label_encoder.classes_\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 1. CONFUSION MATRIX\n",
    "    # ------------------------------------------\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 12})\n",
    "    plt.title('Confusion Matrix: Where does the model fail?', fontsize=14)\n",
    "    plt.xlabel('Predicted Emotion', fontsize=12)\n",
    "    plt.ylabel('Actual Emotion', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 2. CLASSIFICATION REPORT (BAR CHART)\n",
    "    # ------------------------------------------\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    # Filter out 'accuracy', 'macro avg', 'weighted avg' rows to just show classes\n",
    "    df_classes = df.iloc[:-3]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Plotting F1-Score\n",
    "    bars = sns.barplot(x=df_classes.index, y=df_classes['f1-score'], palette='viridis')\n",
    "\n",
    "    plt.title('Performance by Emotion (F1-Score)', fontsize=14)\n",
    "    plt.ylabel('F1 Score (Higher is Better)', fontsize=12)\n",
    "    plt.xlabel('Emotion', fontsize=12)\n",
    "    plt.ylim(0, 1.05)\n",
    "\n",
    "    # Add numbers on top of bars\n",
    "    for i, v in enumerate(df_classes['f1-score']):\n",
    "        bars.text(i, v + 0.02, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('emotion_performance.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Print Text Report for Copy-Pasting\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "id": "eX99CbUjTVTU",
    "outputId": "bf5e86e0-f675-487a-f0a5-49127a02605c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA FROM LOGS\n",
    "# ==========================================\n",
    "epochs = range(1, 16)\n",
    "\n",
    "train_acc = [21.18, 34.29, 46.35, 55.38, 61.98, 74.31, 78.82, 81.51, 83.33, 85.42, 87.50, 92.88, 90.28, 90.36, 92.71]\n",
    "test_acc  = [34.38, 50.00, 60.42, 53.82, 67.71, 75.69, 79.17, 75.00, 85.42, 81.25, 83.68, 86.81, 74.65, 87.50, 79.86]\n",
    "train_loss = [1.7914, 1.4572, 1.4794, 0.9374, 0.9666, 0.7402, 0.4621, 0.4132, 0.1512, 0.3579, 0.1734, 0.1042, 0.1668, 0.0635, 0.1031]\n",
    "\n",
    "# ==========================================\n",
    "# 2. PLOTTING\n",
    "# ==========================================\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# --- Accuracy Plot ---\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_acc, 'b-o', label='Training Accuracy', linewidth=2)\n",
    "plt.plot(epochs, test_acc, 'g-o', label='Test Accuracy', linewidth=2)\n",
    "# Highlight the best epoch (Epoch 14)\n",
    "plt.scatter(14, 87.50, s=200, c='red', marker='*', zorder=5, label='Best Model (87.5%)')\n",
    "plt.title('Model Accuracy: Transformer Learning Curve', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(epochs)\n",
    "\n",
    "# --- Loss Plot ---\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_loss, 'r-o', label='Training Loss', linewidth=2)\n",
    "plt.title('Training Loss: Convergence Over Time', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(epochs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=300) # Saves high-res image for your report\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "F6NAx1mYVAOz",
    "outputId": "ab895ec1-b6e5-45ce-ad55-3d31c5ce1361"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# REPAIR BLOCK: Reconstruct Data and Load Model\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 1. Load the Best Saved Model\n",
    "print(\"Loading your best saved model...\")\n",
    "model, processor, label_encoder, device = load_trained_model()\n",
    "\n",
    "# 2. Re-load Data to reconstruct the Test Split\n",
    "print(\"Reconstructing Test Set (using random_seed=42)...\")\n",
    "file_paths, emotions = load_ravdess_data()\n",
    "\n",
    "# 3. Encode labels using the loaded encoder\n",
    "labels_encoded = label_encoder.transform(emotions)\n",
    "\n",
    "# 4. Split exactly as before (random_state=42 is CRITICAL to get the same test set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    file_paths, labels_encoded,\n",
    "    test_size=config.TEST_SIZE,\n",
    "    random_state=42,\n",
    "    stratify=labels_encoded\n",
    ")\n",
    "\n",
    "# 5. Re-create Dataset and Loader\n",
    "test_dataset = RAVDESSDataset(X_test, y_test, processor)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Test Loader Ready! ({len(X_test)} samples)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# NOW GENERATE THE GRAPHS\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_report_graphs(model, test_loader, device, label_encoder):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"Running inference on test set...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            # Safe mask retrieval\n",
    "            if 'attention_mask' in batch:\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "            else:\n",
    "                attention_mask = torch.ones_like(input_values).to(device)\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_values, attention_mask)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Get Class Names\n",
    "    class_names = label_encoder.classes_\n",
    "\n",
    "    # --- 1. CONFUSION MATRIX ---\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix', fontsize=14)\n",
    "    plt.xlabel('Predicted Emotion')\n",
    "    plt.ylabel('Actual Emotion')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # --- 2. CLASSIFICATION REPORT ---\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df_classes = df.iloc[:-3] # Remove average rows\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = sns.barplot(x=df_classes.index, y=df_classes['f1-score'], palette='viridis')\n",
    "\n",
    "    plt.title('F1-Score by Emotion', fontsize=14)\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.ylim(0, 1.05)\n",
    "\n",
    "    # Add numbers\n",
    "    for i, v in enumerate(df_classes['f1-score']):\n",
    "        bars.text(i, v + 0.02, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('emotion_performance.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nDetailed Report:\\n\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "# Run it!\n",
    "generate_report_graphs(model, test_loader, device, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "id": "xaqGTEyoVjz-",
    "outputId": "0c778286-133d-49db-950f-0962004234ef"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "def plot_roc_curves(model, test_loader, device, label_encoder):\n",
    "    model.eval()\n",
    "\n",
    "    # 1. Get Probabilities\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"Calculating probabilities for ROC curves...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            if 'attention_mask' in batch:\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "            else:\n",
    "                attention_mask = torch.ones_like(input_values).to(device)\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Get Logits -> Softmax\n",
    "            logits = model(input_values, attention_mask)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # 2. Binarize labels for One-vs-Rest ROC\n",
    "    classes = label_encoder.classes_\n",
    "    n_classes = len(classes)\n",
    "    y_test_bin = label_binarize(all_labels, classes=range(n_classes))\n",
    "\n",
    "    # 3. Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], all_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # 4. Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = cycle(['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray'])\n",
    "\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label='{0} (area = {1:0.2f})'.format(classes[i], roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves by Emotion')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig('roc_curves.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Run it\n",
    "plot_roc_curves(model, test_loader, device, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "55jKX8_DVnHW",
    "outputId": "664a2ee6-724a-4782-b1d6-f913650208fa"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_tsne(model, test_loader, device, label_encoder):\n",
    "    model.eval()\n",
    "\n",
    "    # We need to hook into the model to get the \"pooled\" features\n",
    "    # (the layer BEFORE the final classifier)\n",
    "    features = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Hook function to capture inputs to the classifier\n",
    "    def get_features_hook(module, input, output):\n",
    "        # input[0] is the pooled 128-dim vector\n",
    "        features.append(input[0].cpu().numpy())\n",
    "\n",
    "    # Register the hook on the classifier layer\n",
    "    handle = model.classifier.register_forward_hook(get_features_hook)\n",
    "\n",
    "    print(\"Extracting deep features for t-SNE (this takes time)...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            if 'attention_mask' in batch:\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "            else:\n",
    "                attention_mask = torch.ones_like(input_values).to(device)\n",
    "\n",
    "            # This forward pass triggers the hook\n",
    "            model(input_values, attention_mask)\n",
    "            labels_list.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    # Remove hook\n",
    "    handle.remove()\n",
    "\n",
    "    # Flatten list of arrays\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.array(labels_list)\n",
    "\n",
    "    # Run t-SNE\n",
    "    print(\"Computing t-SNE projection (turning 128 dimensions into 2)...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "    features_2d = tsne.fit_transform(features)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    classes = label_encoder.classes_\n",
    "\n",
    "    # Scatter plot for each class\n",
    "    for i, class_name in enumerate(classes):\n",
    "        indices = labels == i\n",
    "        plt.scatter(features_2d[indices, 0], features_2d[indices, 1],\n",
    "                    label=class_name, alpha=0.7, s=30)\n",
    "\n",
    "    plt.legend(title=\"Emotions\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.title('t-SNE Visualization of Emotion Embeddings')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tsne_clusters.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Run it\n",
    "plot_tsne(model, test_loader, device, label_encoder)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
